\section{Introduction and related work}
One of the central assumptions in node classification tasks is that neighboring nodes have similar classes. This has been extensively used in node classification tasks, and in machine learning based approaches to predict node colors. Such approaches are now often denoted by graph machine learning (i.e. machine learning where the input is a graph/network). Three main approaches have been proposed to take advantage of a graph in machine learning:
\begin{itemize}
\item   Regularize the output requiring that neighboring nodes should have similar classes and graph partitioning.
\item   Use the graph to propagate labels and learn the best propagation or use the graph to project the nodes to real valued vectors and use those for supervised or unsupervised learning.
\item   Use graph convolution network to learn the relation between the input of a node and its neighbors to its class.
\end{itemize}

\subsection*{Output regularization}
Methods based on the first approach (regularization of output or graph partitioning) include among others partitioning the graphs based on the eigenvalues of the Laplacian (if nodes with the same partition have similar classes). The Laplacian of a graph is $L = D-A$, where $D$ is a diagonal matrix, with the sum of each row in the diagonal and $A$ the adjacency matrix. This Laplacian is often weighted by multiplying it by $D^\frac{1}{2}$ on the left and the right to normalize for degree \cite{dhillon2007weighted}\cite{karypis1995metis}.  Other works have used variants of this idea, each using smoothness and graph distance differently (e.g., \cite{belkin2004semi}\cite{sindhwani2005beyond}). An alternative approach is to use quadratic penalty with fixed labels for seed nodes \cite{zhou2004learning}\cite{zhu2003semi}.

\subsection*{Node Projection and propagation}
For the second approach (diffusion of labels), multiple diffusion and information propagation models have been proposed \cite{rosenfeld2017semi}; 
for example, DeepWalk \cite{perozzi2014deepwalk}, where a truncated random walk is performed on nodes. It then uses these sentences as an input to skipgram to compute a projection of each word into $\mathbb{R}^N$ maximizing the sentence probability.  
Planetoid \cite{yang2016revisiting} also uses random walks combined with negative sampling. Duvenaud et al .  used a translation of subgraphs to hash functions for a similar task in the context of molecule classifications \cite{duvenaud2015convolutional}. A very similar approach was presented by Leskovech by projecting nodes minimizing the distance of neighbored nodes in a truncated random walk (Node2vec \cite{grover2016node2vec}). The DNGR model \cite{cao2016deep} uses random walk to compute the mutual information between points (the PPMI-positive pointwise mutual information), and then a SVD decomposition to project into space. PPMI is a measure often used in information theory. PPMI was used for word  representations in \cite{levy2015improving} and is a sparse high dimensional representation. Another possible approach is the projection of the input over the Laplacian eigenvectors, and the usage of the projection for classification, where either the graph itself is used (in such a case, the eigenvectors themselves are used) or an input to the graph was used. In such a case, a convolution with these eigenvectors was used \cite{masci2015shapenet}\cite{Monti2017} A Multi-Dimensional-Scaling (MDS) projection of the points in the graphs was also used for a similar goal \cite{belkin2002laplacian}\cite{levy2015improving}. An alternative approach was inspired by word embedding methods \cite{mikolov2013distributed} such as word2vec. These methods use the graph to define a “context” in relation to which the node embedding is constructed. When the data includes only the graph, the embeddings are used as features and fed into existing predictors \cite{perozzi2014deepwalk}. When the data includes node features, these embedding are used as an additional regularization term to a standard loss over the labeled nodes \cite{thomas2016semi}\cite{yang2016revisiting}. As such, these methods can be thought of as propagating features rather than labels.  Refex \cite{henderson2011s} defines local features to translate each node to a vector features and use those to predict classes.

\subsection*{Graph convolutional network}
Recently, Kipfs and collaborators, in a seminal work, propose a simplification of spectral based convolutions \cite{thomas2016semi}\cite{schlichtkrull2017modeling}, and instead use a two-layer approach, which can be summarized as: $softmax(\tilde{A}\cdot ReLU(\tilde{A}\cdot X\cdot W_{1})\cdot W_{2})$, where $\tilde{A}$ is a normalized adjacency matrix $\tilde{A}=D^{-\frac{1}{2}}\cdot A\cdot D^{-\frac{1}{2}}$. They test their work on multiple graphs with labeled nodes including CiteSeer, Cora, Pubmed and Nell.  Convolution approaches can also be used where the graph is used as a filter on the input. Most such convolutions are spectral (use the Laplacian eigenvectors). However, recent methods are based on random filters. Such convolutions include among other:  Atwood et al. \cite{Atwood2016}, which define predetermined convolutions with powers of the adjacency matrix and then combine these powers using learned weights to maximize the classification precision of either the full graph or the classification of nodes. Bruna et al.  \cite{bruna2013spectral} provide a multi-level graph convolution with pooling, where at each stage nodes are merged into clusters using agglomerative clustering methods, and combine it with a pooling method to represent the different resolution of images. This has been extended \cite{henaff2015deep}\cite{bronstein2016geometric}  to different convolutional kernels (mainly spectral, but also diffusion based kernels) and the classification of images, using ImageNet (see \cite{bronstein2016geometric} for a detailed review of all convolution methods).  Vandergheynst and collaborators (see again review in \cite{bronstein2016geometric}) mainly use polynomial convolution in the spectral domain. Similar formalisms were used to study not only single snapshots, but also using recurrent networks time series of graphs, mainly again in image analysis \cite{Seo2018}.

\subsection*{Proposed approach}
A weakness of most of these approaches is the usage of the graph only as a similarity measure and ignoring any more complex features of the topology of directed graphs, focusing on the above-mentioned assumption that proximity in the graph implies similarity in labels. 
The main advances presented here are:
\begin{itemize}
\item   The directed graph topology-of each node contains information on its class and can be used to predict the class.
\item   When the topology is combined with information propagation and convolution of external information, the combination outperforms all current methods.
\item   Even in the absence of external information, combining topology and propagation can produce an excellent classification. 
\item   We propose an adaptation to the Kipf and Welling formalism to show that topological features contain enough information to classify nodes. We then show that when combined with content, such methods outperform the state of the art methods, leading to a new method to classify nodes using GCN. This adaptation includes the introduction of both the symmetric and anti-symmetric components of the adjacency matrix in the learning step.
\end{itemize}


% One of the central assumptions in node classification tasks is that neighboring nodes have similar classes. This has been extensively used in node classification tasks, and in machine learning based approaches to predict node colors. Such approaches are now often denoted as graph machine learning (i.e. machine learning where the input is a graph/network, in contrast with neural network, where the network topology is not directly related to the observations). Three main approaches have been proposed to take advantage of a graph in machine learning:
% \begin{itemize}
% \item Regularization of the output requiring that neighboring nodes should have similar classes and graph partitioning. Such method often include partitioning the graphs based on the eigenvalues of the Laplacian or weighted Laplacian (Dhillon, Guan et al. 2007) (Karypis and Kumar 1995).  Other works have used variants of this idea, each using smoothness and graph distance differently (e.g., (Belkin and Niyogi 2004) (Sindhwani, Niyogi et al. 2005)). An alternative approach is to use quadratic penalty on the difference between neighboring nodes (Zhou, Bousquet et al. 2004) (Zhu, Ghahramani et al. 2003).  
% \item	Graph based label propagation. Multiple diffusion and information propagation models have been proposed (Rosenfeld and Globerson 2017). For example, DeepWalk (Perozzi, Al-Rfou et al. 2014), where a truncated random walk is performed on nodes and used to project nodes to a multidimensional real space.  Planetoid (Yang, Cohen et al. 2016) also uses random walks combined with negative sampling. Duvenaud et al.  used a translation of subgraphs to hash functions for a similar task in the context of molecule classifications (Duvenaud, Maclaurin et al. 2015). A very similar approach was presented by Leskovech by projecting nodes minimizing the distance of neighbored nodes in a truncated random walk (Node2vec (Grover and Leskovec 2016)). The DNGR model (Cao, Lu et al. 2016) uses random walk to compute the mutual information between points  A Multi-Dimensional-Scaling (MDS) projection of the points in the graphs was also used for a similar goal (Belkin and Niyogi 2002, Levy, Goldberg et al. 2015). An alternative approach was inspired by word embedding methods (Mikolov, Sutskever et al. 2013) such as word2vec. These methods use the graph to define a “context” in relation to which the node embedding is constructed. When the data includes only the graph, the embeddings are used as features and fed into existing predictors (Perozzi, Al-Rfou et al. 2014). When the data includes node features, these embedding are used as an additional regularization term to a standard loss over the labeled nodes (Kipf and Welling 2016, Yang, Cohen et al. 2016). As such, these methods can be thought of as propagating features rather than labels.  Refex (Henderson, Gallagher et al. 2011) defines local features to translate each node to a vector features and use those to predict classes. 
% \item	Graph Convolution Networks (GCN) to learn the relation between the input of a node and its neighbors to its class. Recently, Kipfs and collaborators, in a seminal work, propose a simplification of spectral based convolutions (REF Kipf and Welling 2016, Schlichtkrull, Kipf et al. 2017), and instead use a two-layer approach, where the weights of each layer are multiplied bya  derivative of the adjancecy matrix. They test their work on multiple graphs with labeled nodes including CiteSeer, Cora,Pubmed and Nell.  Convolution approaches can also be used where the graph is used as a filter on the input. Most such convolutions are spectral (use the Laplacian eigenvectors). However, recent methods are based on random filters (REF Atwood and Towsley 2016)(Bruna, Zaremba et al. 2013) (Henaff, Bruna et al. 2015, Bronstein, Bruna et al. 2016). Similar formalisms were used to study not only single snapshots, but also time series of graphs, mainly again in image analysis (Seo, Defferrard et al. 2016).
% \end{itemize}
% These methods presume a single graph or a single multi graphs and static vertex classifications. However, in many realistic scenarios, both the graph and the labels evolve. An interesting example would be companies labeled as succesful or not, and graphs representing the relation between such companies.  We here analyze a companies similairity network over 17 years and show that a combination of GCN with LSTM can improve the accuracy of the prediction of bankrupcies and 