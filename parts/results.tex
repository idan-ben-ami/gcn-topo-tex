\section{Results}

A typical example for node classification is the classification of the category of a manuscript from its content as characterized for example through its Bag-of-Words (BOW) description. Two standards test sets used for this goal are the Cora and CiteSeer citation networks \cite{giles1998citeseer}. The current state of the art method to predict the category of a paper is through GCN, where the input for each node in the citation network is its Bag of Words (BOW), and the output of each layer is a function of the product of the adjacency matrix with the input and with weights optimized by the network: $X_{n+1}=\sigma(A\cdot X_{n}\cdot W_{n})$
where $A$ is the adjacency matrix, $W_{n}$ are the weights from the $n'th$ layer, $X_{n}$ are the values of the nodes in this layer, and $\sigma(x)$ is a non-linear function. The input $X_{o}$ is a BOW and the output of the last layer is a soft-max used to determine the probability of each category given the input. The adjacency matrix is typically replaced by a normalized and symmetrized version (e.g., $D^{-\frac{1}{2}}\cdot (A+A^{T}+I)\cdot D^{-\frac{1}{2}}$, Other versions of GCN with similar concepts have also been proposed \cite{schlichtkrull2017modeling}\cite{masci2015shapenet}\cite{henaff2015deep}\cite{duvenaud2015convolutional}\cite{thomas2016semi}. 
While highly precise, such methods presume the existence of external information on the nodes, such as the BOW of each manuscript. In the absence of any such information, we propose that combining the relation between the node topology and its class with existing methods of class propagation can produce a similar classification accuracy. 
To test that neighbor class and the node self-topology (as shall be further defined) are correlated with the node class, we produced two tests. We first computed the relative frequency of classes in neighbors, given the class of the node: ($\frac{P(neighbor\:has\:class\:i│current\:node\:has\:class\:j)}{P(node\:has\:class\:i)}$  (Fig \ref{fig:kruskal}C). In the absence of correlations, one would expect a flat value, while an absolute correlation would produce an identity matrix. In the Cora or Citeseer networks, the mass of the diagonal is 60 \% of the mass (compared with an expected 15 \% (Fig \ref{fig:kruskal}C)).
To test for the relation between node topology and class, we computed the average value of multiple topological features (Table \ref{tab:features}) for nodes with a given class (in the current context manuscripts belonging to a certain field). One can see (Fig \ref{fig:kruskal}B) that the average of many features varies according to the class of the node. When a Kruskal Wallis non-parametric test is performed to test for the relation between the node class (manuscript field) and the distribution of features, most features are associated with the node class (Fig \ref{fig:kruskal}A). 
To test that topology and information propagation can be used to classify node classes, we introduced the topological features above, and the number of neighbors belonging to the training set with a given class as input to a Feed-Forward Network. These two types of information by themselves can be used to classify the class of nodes quite precisely (Fig \ref{fig:acc_per}).
To avoid having to explicitly compute topological features, which can be computationally expensive, a direct computation of the topology can be proposed. A simple way to describe such local features is though operations on products of the adjacency matrix. For example, the number of triangles $i\rightarrow j,i\rightarrow k,j\rightarrow k$, are the product of $a_{ij}$ and $b_{ik}$; $B=A\cdot A^{T}$ (Fig \ref{fig:adj_example}). Thus, instead of explicitly computing such features, one can use as input to the FFN combinations of these products on a one-hot representation of the training set class. Formally, let us define for node $i$, the vector  $v_{i}$, where $v_{i}^{j}$ is the number of neighbors of node $i$ that are in the training set and are of class $j$, and $V$ is the matrix of all vectors $v_{i}$. To this, we add a last constant value to the vector, as shall be explained. We then use different combinations of $A\cdot V, A^{T}\cdot V, A\cdot A^{T}\cdot V$…. as inputs to an FFN (see Methods). 
When these products are applied to the last row (a constant value), they simply count sub-graphs. However, when multiplied by the other component, the sub-graphs composed of a specific class are counted (Fig \ref{fig:adj_example}). The accuracy obtained for such products outperforms the only explicit topological measures, or information propagation (Fig. \ref{fig:acc_per}).
Such a formalism is easily amenable to a graph convolution network, by using $V$ as the input, but incorporating also the sign of second neighbors (i.e.,  $v_{i}^{j}$  would be the number of second neighbors of node $i$ that are in the training set and are of class $j$). Then instead of performing the product of $A$ with $V$, we use $V$ as the input a convolutional neural network. Note that such input can then be combined with other inputs, such as the BOW, used by Kipf et al. Also., to incorporate the products of both $A$ and  $A^{T}$, some reshaping must be performed within the network (see methods).

\subsection*{Combination of BOW and topology outperforms BOW}
To test that adding such combinations improves the accuracy compared with state of the art methods, we have compared the accuracy of a network as proposed by Kipf et al. \cite{thomas2016semi} (we have validated through an extensive grid search that the parameters proposed by Kipf et al. are indeed optimal for this architecture), with a similar network using as an input a concatenation of the BOW and the neighbor and second neighbor test category frequency. Indeed (Fig \ref{fig:acc_comp}), the combination of the topological propagation with the BOW improves the accuracy for all train sizes tested (except for the 15\% and 25\% training fraction where no significant difference was observed). Note that the random accuracy in the Cora dataset is 20\% and there is an inherent level of arbitrariness in the definition of scientific domains and thus the precision of 100\% cannot be reached.
While combining neighbor information with BOW outperforms the state of the art methods, the neighbor information by itself produces results not far from that (less than 5\%  below) (Fig \ref{fig:acc_comp}). Moreover, for large training fractions, the purely topological outperforms the BOW based classification. Thus, even in the absence of any external information, high precision can be obtained on node class classification, just by combining the network topology and the classification of neighboring nodes. 

\subsection*{The direction of edges is important}
In contrast with images, directed networks have a complex topology where the direction of edges is of importance. Thus, explicitly incorporating the adjacency matrix and its transpose in the convolutional network is expected to produce better results than the symmetrized and self-edged adjacency matrix($(\frac{A+A^{T}}{2}+I)$. To test for that, we compared the performance of the two networks (each normalized as above). In all training set fractions, the full representation outperforms the symmetric representation or has equal accuracy (Fig \ref{fig:acc_comp}). Thus, it is not only the density of a class in the environment contributing to the probability of having the same class but the explicit topology of the network. We have further tested whether explicitly adding the topological features to the input (instead of letting the network deduce the optimal network combination) would improve the accuracy. However, no difference was observed between the results with and without the explicit topological features.